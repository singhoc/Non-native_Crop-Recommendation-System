{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_x4U1ZIyeaV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsdMLymim9PY"
      },
      "outputs": [],
      "source": [
        "#temp,dew,humidity,cloud_cover,solar_radiation,solar_energy, uv_radiation\n",
        "# Define initial min and max values\n",
        "min_val = [1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
        "max_val = [-1000, -1000, -1000, -1000, -1000, -1000, -1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H3PjXVQUHFF"
      },
      "outputs": [],
      "source": [
        "def adjust_dataframe_length(df, target_length):\n",
        "    # Check if df.shape[0] is less than the target length\n",
        "    if df.shape[0] < target_length:\n",
        "        df = pd.concat([df, df.tail(target_length - df.shape[0])])\n",
        "\n",
        "    # Check if df.shape[0] is greater than the target length\n",
        "    if df.shape[0] > target_length:\n",
        "        # Create a list of rows where the 4th column (precip) value is 0\n",
        "        zero_precip_rows = []\n",
        "        for i in range(len(df)):\n",
        "            # if df.iloc[i, 3] == 0:\n",
        "            zero_precip_rows.append(i)\n",
        "\n",
        "        # Randomly select rows from the list of zero_precip_rows\n",
        "        rows_to_remove = random.sample(zero_precip_rows, df.shape[0] - target_length)\n",
        "\n",
        "        # Remove the selected rows from the DataFrame\n",
        "        df = df.drop(rows_to_remove)\n",
        "    return df\n",
        "\n",
        "# Normalize the data using Min-Max scaling\n",
        "def min_max_scaling(df, min_val, max_val):\n",
        "    for i, col in enumerate(df.columns):\n",
        "        df[col] = (df[col] - min_val[i]) / (max_val[i] - min_val[i])\n",
        "    return df\n",
        "\n",
        "def append_rows_to_single_row(df):\n",
        "    # Normalize each column separately\n",
        "    # Normalize the data using Min-Max scaling\n",
        "    df_normalized = min_max_scaling(df, min_val, max_val)\n",
        "\n",
        "    # Create an empty dictionary to store the data\n",
        "    data_dict = {}\n",
        "\n",
        "    # Number of days\n",
        "    num_days = len(df_normalized)\n",
        "\n",
        "    # Iterate over each row\n",
        "    for i in range(num_days):\n",
        "        # Get the data from the current row\n",
        "        row_data = df_normalized.iloc[i].values\n",
        "\n",
        "        # Append the data to the dictionary with modified column names\n",
        "        for j, value in enumerate(row_data):\n",
        "            column_name = f\"{df_normalized.columns[j]}_day_{i+1}\"\n",
        "            data_dict[column_name] = value\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    single_row_df = pd.DataFrame([data_dict])\n",
        "\n",
        "    return single_row_df\n",
        "\n",
        "def calculate_min_max(df):\n",
        "    global min_val, max_val\n",
        "    for i, col in enumerate(df.columns):\n",
        "        min_val[i] = min(df[col].min(), min_val[i])\n",
        "        max_val[i] = max(df[col].max(), max_val[i])\n",
        "\n",
        "\n",
        "def load_and_adjust_data(urls, target_length):\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for url in urls:\n",
        "        # Read Excel file into DataFrame\n",
        "        df = pd.read_excel(url)\n",
        "        # Fill NaN values with column means\n",
        "        df= df.drop(columns=['precip'])\n",
        "        df = df.fillna(df.mean())\n",
        "        if df.isna().any().any():\n",
        "            print(\"NaN values encountered in DataFrame from URL:\", url)\n",
        "        #calculating for min max scaling\n",
        "        calculate_min_max(df)\n",
        "        # Adjust the length of the DataFrame\n",
        "        df = adjust_dataframe_length(df, target_length)\n",
        "        # Append the adjusted DataFrame to the list\n",
        "        dfs.append(df)\n",
        "    return dfs\n",
        "\n",
        "def extract_label_from_url(url):\n",
        "\n",
        "    filename = url.split('/')[-1]  # Get the filename from the URL\n",
        "    label = re.split('_\\d+', filename)[1]  # Extract the label based on the pattern\n",
        "    return label[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ql_r11BtIJ-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBNQZq3Os662"
      },
      "outputs": [],
      "source": [
        "mydrive_path = \"/content/drive/MyDrive/ACPS_Project/Modified for model\"\n",
        "\n",
        "# List files in MyDrive with full paths\n",
        "urls = [os.path.join(mydrive_path, file) for file in os.listdir(mydrive_path)]\n",
        "print(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtQKPppeZvoK"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "for url in urls:\n",
        "  labels.append(extract_label_from_url(url))\n",
        "# Load and adjust the data\n",
        "dfs = load_and_adjust_data(urls, 180)\n",
        "\n",
        "# Convert each DataFrame to a single row\n",
        "single_row_dfs = [append_rows_to_single_row(df) for df in dfs]\n",
        "\n",
        "# Concatenate the single-row DataFrames\n",
        "df_concatenated = pd.concat(single_row_dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQX8MATLf3gQ"
      },
      "outputs": [],
      "source": [
        "print(min_val)\n",
        "print(max_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXwAiziKSqy7"
      },
      "outputs": [],
      "source": [
        "# Check if there are any NaN values in the DataFrame\n",
        "if df_concatenated.isna().any().any():\n",
        "    # Print the rows with NaN values\n",
        "    print(\"Rows with NaN values:\")\n",
        "    print(df_concatenated[df_concatenated.isna().any(axis=1)])\n",
        "# Drop rows with NaN values\n",
        "df_concatenated = df_concatenated.dropna()\n",
        "\n",
        "# Print the DataFrame after dropping rows\n",
        "print(df_concatenated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-gtrkFCyT-k"
      },
      "outputs": [],
      "source": [
        "#Split the data into train and test sets\n",
        "X_train, X_test = train_test_split(df_concatenated.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training data into train and validation sets\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "\n",
        "# Define the input layers\n",
        "input_layer_train = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "input_layer_test = Input(shape=(X_test.shape[1], X_test.shape[2]))\n",
        "input_layer_val = Input(shape=(X_val.shape[1], X_val.shape[2]))\n",
        "\n",
        "conv1 = Conv1D(512,kernel_size=3,activation='relu')(input_layer_train)\n",
        "conv2 = Conv1D(512,kernel_size=3,activation='relu')(conv1)\n",
        "conv3 = Conv1D(128,kernel_size=5,activation='relu')(conv2)\n",
        "conv4 = Conv1D(128,kernel_size=5,activation='relu')(conv3)\n",
        "flatten = Flatten()(conv4)\n",
        "\n",
        "# Define the encoding layers\n",
        "encoding_layer1 = Dense(512, activation='relu')(flatten)\n",
        "encoding_layer2 = Dense(256, activation='relu')(encoding_layer1)\n",
        "encoding_layer3 = Dense(128, activation='relu')(encoding_layer2)\n",
        "\n",
        "# Define the decoding layers\n",
        "#decoding_layer1 = Dense(128, activation='relu')(encoding_layer3)\n",
        "decoding_layer2 = Dense(256, activation='relu')(encoding_layer3)\n",
        "decoding_layer3 = Dense(512, activation='relu')(decoding_layer2)\n",
        "decoding_layer4 = Dense(X_train.shape[1], activation='relu')(decoding_layer3)\n",
        "\n",
        "# Define the autoencoder model\n",
        "autoencoder = Model(inputs=input_layer_train, outputs=decoding_layer4)\n",
        "\n",
        "# Compile the autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "\n",
        "# Train the autoencoder on the training data\n",
        "history = autoencoder.fit(X_train, X_train, epochs=1024, validation_data=(X_val, X_val))\n",
        "\n",
        "\n",
        "# Evaluate the autoencoder on the test set\n",
        "reconstruction_error_test = autoencoder.evaluate(X_test, X_test)\n",
        "print(\"Reconstruction error on test set:\", reconstruction_error_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL9po27h5z1a"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6gWWZssLYl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmgFHqoC6g6i"
      },
      "outputs": [],
      "source": [
        "# Plot the reconstruction error for both training and test sets\n",
        "reconstruction_error_train = autoencoder.evaluate(X_train, X_train)\n",
        "print(\"Reconstruction error on training set:\", reconstruction_error_train)\n",
        "# Evaluate the autoencoder on the test set\n",
        "reconstruction_error_test = autoencoder.evaluate(X_test, X_test)\n",
        "print(\"Reconstruction error on test set:\", reconstruction_error_test)\n",
        "plt.bar(['Training', 'Test'], [reconstruction_error_train, reconstruction_error_test])\n",
        "plt.title('Reconstruction Error')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRCnpLrhQr52"
      },
      "outputs": [],
      "source": [
        "labels = np.array(labels)\n",
        "unique_labels = np.unique(labels)\n",
        "print(unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaSu1hw8Xeyw"
      },
      "outputs": [],
      "source": [
        "# Map each label to a numerical value\n",
        "label_mapping = {'jowar': 0, 'rice': 1, 'wheat': 2, 'arhar':3, 'bajra':4, 'barley':5, 'gram':6,'maize':7, 'ragi':8, 'urad':9}\n",
        "\n",
        "# Convert labels to numerical values\n",
        "numerical_labels = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_concatenated.values, numerical_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_model = Model(inputs=input_layer_train, outputs=encoding_layer3)\n",
        "\n",
        "# Extract features from the training, validation, and test sets\n",
        "encoded_train = encoder_model.predict(X_train)\n",
        "encoded_val = encoder_model.predict(X_val)\n",
        "encoded_test = encoder_model.predict(X_test)\n",
        "\n",
        "# Now you can use these encoded representations for further classification tasks\n",
        "inputs = Input(shape=(encoded_train.shape[1],))\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "xy = Dense(32, activation='relu')(x)\n",
        "outputs = Dense(10, activation='softmax')(xy)\n",
        "\n",
        "classifier_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the classifier model\n",
        "classifier_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the classifier model\n",
        "classifier_history = classifier_model.fit(encoded_train, y_train, epochs=200, validation_data=(encoded_val, y_val))\n",
        "\n",
        "# Evaluate the classifier model on the test set\n",
        "classifier_loss, classifier_accuracy = classifier_model.evaluate(encoded_test, y_test)\n",
        "print(\"Classifier accuracy on test set:\", classifier_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvbSM0rX9GD"
      },
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "plt.plot(classifier_history.history['loss'], label='Training Loss')\n",
        "# Plot validation loss\n",
        "plt.plot(classifier_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Classifier Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(classifier_history.history['accuracy'], label='Training Accuracy')\n",
        "# Plot validation accuracy\n",
        "plt.plot(classifier_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Classifier Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uManXUaUbGze"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "predictions = classifier_model.predict(encoded_test)\n",
        "\n",
        "# Convert predicted probabilities into class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "accuracy = np.mean(predicted_labels == y_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUogNNygegBn"
      },
      "outputs": [],
      "source": [
        "def predict_categories_from_urls(urls, encoder_model, classifier_model):\n",
        "    # Load and adjust the data\n",
        "    datas = load_and_adjust_data(urls, 180)\n",
        "\n",
        "    # Convert each DataFrame to a single row\n",
        "    single_row_datas = [append_rows_to_single_row(data) for data in datas]\n",
        "\n",
        "    # Concatenate the single-row DataFrames\n",
        "    datas_concatenated = pd.concat(single_row_datas, axis=0)\n",
        "\n",
        "    # Use the trained encoder model to extract features from the reconstructed data\n",
        "    encoded_reconstructed_data = encoder_model.predict(datas_concatenated.values.reshape(datas_concatenated.shape[0], datas_concatenated.shape[1], 1))\n",
        "\n",
        "    # Make predictions on the reconstructed data using the classifier model\n",
        "    predictions = classifier_model.predict(encoded_reconstructed_data)\n",
        "\n",
        "    # Convert predicted probabilities into class labels\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Map the class labels to their corresponding categories\n",
        "    category_mapping = {0: 'jowar', 1: 'rice', 2: 'wheat',3:'arhar',4: 'bajra',5: 'barley',6: 'gram',7: 'maize',8: 'ragi',9:'urad'}\n",
        "    predicted_categories = [category_mapping[label] for label in predicted_labels]\n",
        "\n",
        "    return predicted_categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7Z1_t9pehn3"
      },
      "outputs": [],
      "source": [
        "test_urls = ['/content/drive/MyDrive/ACPS_Project/Modified for model/Adilabad_2015_wheat_10-05_weather.xlsx']\n",
        "predicted_categories = predict_categories_from_urls(test_urls, encoder_model, classifier_model)\n",
        "print(\"Predicted categories:\", predicted_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRl2EPoIge7t"
      },
      "outputs": [],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZhAuM3agknI"
      },
      "outputs": [],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiNO4y5ag1J-"
      },
      "outputs": [],
      "source": [
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEfheRryYLSF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assuming your model is already defined and trained\n",
        "model = classifier_model\n",
        "automodel = encoder_model\n",
        "# model.save('/content/my_model/classifier.h5')\n",
        "automodel.save('/content/drive/MyDrive/encoder_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZpoTgurjF7a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def excel_to_json(excel_file_path):\n",
        "    # Read Excel file into DataFrame\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "\n",
        "    # Drop the 'precip' column\n",
        "    df.drop(columns=['precip'], inplace=True)\n",
        "\n",
        "    # Fill NaN values with column means\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    # Convert DataFrame to JSON\n",
        "    json_data = df.to_json(orient='records',force_ascii=False)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "excel_file_path = '/content/drive/MyDrive/ACPS_Project/Modified for model/Adilabad_2015_wheat_10-05_weather.xlsx'\n",
        "json_data = excel_to_json(excel_file_path)\n",
        "print(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install visualkeras"
      ],
      "metadata": {
        "id": "J5nV7SvE4Eue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIi8vWVKjdw8"
      },
      "outputs": [],
      "source": [
        "import visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/paulgavrikov/visualkeras"
      ],
      "metadata": {
        "id": "E0xTV7-t6fzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFont\n",
        "\n",
        "font = ImageFont.load_default(128)"
      ],
      "metadata": {
        "id": "gqs4iyOy73z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import visualkeras\n",
        "\n",
        "model = classifier_model\n",
        "\n",
        "visualkeras.layered_view(model,draw_volume=False, legend=True).show() # display using your system viewer\n",
        "visualkeras.layered_view(model, to_file='output.png', legend=True, font =font) # write to disk\n",
        "visualkeras.layered_view(model, to_file='output.png', legend=True).show() # write and show"
      ],
      "metadata": {
        "id": "Z0Y_ihJs5ain"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}